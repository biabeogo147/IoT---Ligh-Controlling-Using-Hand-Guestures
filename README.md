# Light Control Using Hand Gestures ğŸ–ï¸ğŸ’¡

![Hand Gesture Recognition](./res/hand_gesture_recognition.jpeg)

This project enables users to control lights using hand gestures, integrating Google's MediaPipe Gesture Recognizer with a deep learning model for real-time gesture classification. The system supports both simulated environments and physical lighting devices via a Modbus RTU RS485 relay module.

## Key Features âœ¨

- **Real-time Gesture Recognition** ğŸ•¹ï¸: Detects hand landmarks from a webcam feed using MediaPipe Gesture Recognizer.
- **Deep Learning Classification** ğŸ§ : Utilizes a Multi-Layer Perceptron (MLP) to classify gestures into light control commands.
- **Flexible Control** âš™ï¸: Manages lights in simulated or physical setups using a 4-channel Modbus RTU RS485 relay module.
- **Customizable Gestures** ğŸ“: Define gestures and actions in the `hand_gesture.yaml` file for easy modification.
- **Automated Data Collection** ğŸ“Š: Simplifies gesture data collection with `generate_landmark_data.py`.

## Project Structure ğŸ“

```
Light-Control-Using-Hand-Gestures/
â”œâ”€â”€ data/                       # Landmark data (train, val, test CSV files) ğŸ“ˆ
â”œâ”€â”€ img/                        # README images ğŸ–¼ï¸
â”œâ”€â”€ models/                     # Trained deep learning model ğŸ¤–
â”œâ”€â”€ sign_imgs/                  # Sample gesture images âœ‹
â”œâ”€â”€ .gitignore                  # Git ignore file ğŸ™ˆ
â”œâ”€â”€ Light Controlling Using Hand Gestures.pdf # Project guide ğŸ“„
â”œâ”€â”€ README.md                   # This file ğŸ“–
â”œâ”€â”€ controller.py               # Physical light control logic ğŸ’¡
â”œâ”€â”€ detect_simulation.py        # Simulated gesture-based control ğŸ®
â”œâ”€â”€ generate_landmark_data.py   # Gesture data collection script ğŸ“·
â”œâ”€â”€ hand_gesture.yaml           # Gesture-action configuration âš™ï¸
â”œâ”€â”€ model.py                    # Deep learning model architecture ğŸ§ 
â”œâ”€â”€ train.py                    # Model training script ğŸ‹ï¸
â”œâ”€â”€ utils.py                    # Utility functions ğŸ› ï¸
```

## Getting Started ğŸš€

### 1. Configure Gestures âš™ï¸

Edit `hand_gesture.yaml` to define gestures and their corresponding light control actions:

```yaml
gestures:
  0: "turn_off"  # âœ‹ Turn off lights
  1: "light1"    # ğŸ’¡ Light 1
  2: "light2"    # ğŸ’¡ Light 2
  3: "light3"    # ğŸ’¡ Light 3
  4: "turn_on"   # ğŸŒŸ All lights on
```

### 2. Collect Gesture Data ğŸ“·

Run the data collection script to gather hand landmark data:

```bash
  python generate_landmark_data.py
```

- A webcam window will open. Press a key (e.g., 'a' for class 0, 'b' for class 1) to start/stop recording a gesture.
- Perform the gesture in front of the camera.
- Repeat for all gestures, then press 'q' to exit.
- Data is saved as `landmark_train.csv`, `landmark_val.csv`, and `landmark_test.csv` in the `data/` directory.

### 3. Train the Model ğŸ‹ï¸

Train the gesture classification model:

```bash
  python train.py
```

The trained model is saved in the `models/` directory.

### 4. Run Simulation ğŸ®

Test gesture-based light control in a simulated environment:

```bash
  python detect_simulation.py
```

The webcam will detect gestures and display corresponding actions on-screen.

### 5. Control Physical Lights ğŸ’¡

For physical light control, connect a 4-channel Modbus RTU RS485 relay module and a USB to RS485 converter, then run:

```bash
  python controller.py
```

Ensure the COM port and Modbus address in `controller.py` match your hardware setup.

## Gestures and Actions ğŸ“‹

![Action Class](./res/action-class.jpeg) 